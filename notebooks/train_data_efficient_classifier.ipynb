{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Volbis/cough_ai/blob/main/notebooks/train_data_efficient_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fgVWTMK9SNz"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Classifying Pneumonia with HeAR and COUGHVID Dataset\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/Volbis/cough_ai/blob/main/notebooks/train_data_efficient_classifier.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>  \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/Volbis/cough_ai/blob/main/notebooks/train_data_efficient_classifier.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://huggingface.co/google/hear\">\n",
        "      <img alt=\"Hugging Face logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on Hugging Face\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n",
        "\n",
        "\n",
        "This Colab notebook demonstrates how to use the HeAR (Health Acoustic Representations) model, directly from Hugging Face, to create and utilize embeddings from health-related audio for pneumonia classification. The notebook focuses on building a data-efficient pneumonia classifier system using the COUGHVID dataset from KaggleHub.\n",
        "\n",
        "Embeddings are compact, numerical representations of audio data that capture important features, making them suitable for training machine learning models with limited data and computational resources. Learn more about embeddings and their benefits at [this page](https://developers.google.com/health-ai-developer-foundations/hear).\n",
        "\n",
        "#### Here's a breakdown of the notebook's steps:\n",
        "\n",
        "1.  **Model Loading:** The HeAR model is loaded from the Hugging Face Hub (requires authentication with your Hugging Face account).\n",
        "\n",
        "2.  **Dataset Loading:**\n",
        "    *   **COUGHVID Dataset:** Audio files and labels are downloaded from KaggleHub (orvile/coughvid-v3 dataset).\n",
        "    *   **Label Extraction:** Pneumonia labels are extracted from the CSV file, considering COVID-19 and lower respiratory infections as pneumonia cases.\n",
        "\n",
        "3.  **Embedding Generation:**\n",
        "    *   **Preprocessing:** Audio files are loaded and processed using `librosa`, resampled to 16kHz (required by HeAR model) and segmented into 2-second clips.\n",
        "    *   **Inference:** The preprocessed 2-second audio clips are fed to the HeAR model to generate embeddings. Each clip produces a 512-dimensional HeAR embedding vector.\n",
        "    *   **Visualization (Optional):** The notebook includes functions to display the audio waveform, Mel spectrogram, and an audio player for each file and its individual clips.\n",
        "\n",
        "4.  **Classifier Training:**\n",
        "    *   **Train/Test Split:** Data is split into training and testing sets with stratification based on pneumonia labels.\n",
        "    *   **Model Selection:** Several scikit-learn classifiers are trained, including:\n",
        "        *   Support Vector Machine (linear kernel)\n",
        "        *   Logistic Regression\n",
        "        *   Gradient Boosting\n",
        "        *   Random Forest\n",
        "        *   Multi-layer Perceptron (MLP)\n",
        "    *   **Training:** Each classifier is trained using the generated HeAR embeddings and pneumonia labels.\n",
        "\n",
        "5.  **Pneumonia Classification:**\n",
        "    *   **Evaluation:** Trained classifiers are evaluated on the test set.\n",
        "    *   **Prediction:** The models predict whether audio clips contain signs of pneumonia.\n",
        "\n",
        "6.  **Embedding Visualization:**\n",
        "    *   **PCA Plot:** A plot visualizing the data points in a PCA space, colored by pneumonia status.\n",
        "    *   **Barcode Visualization:** The embeddings are visualized as \"barcodes\" showing the magnitude of each dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uZFXCSuqr1V"
      },
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDWy-twvwC2d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "from IPython.display import Audio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"soundfile\")\n",
        "warnings.filterwarnings(\"ignore\", module=\"librosa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWwF_J5WwC2d"
      },
      "source": [
        "## Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGnywzjKwC2d"
      },
      "outputs": [],
      "source": [
        "def plot_waveform(sound, sr, title, figsize=(12, 4), color='blue', alpha=0.7):\n",
        "    \"\"\"Plots the waveform of the audio using librosa.display.\"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    librosa.display.waveshow(sound, sr=sr, color=color, alpha=alpha)\n",
        "    plt.title(f\"{title}\\nshape={sound.shape}, sr={sr}, dtype={sound.dtype}\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_spectrogram(sound, sr, title, figsize=(12, 4), n_fft=2048, hop_length=256, n_mels=128, cmap='nipy_spectral'):\n",
        "    \"\"\"Plots the Mel spectrogram of the audio using librosa.\"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=sound, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "    librosa.display.specshow(log_mel_spectrogram, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', cmap=cmap)\n",
        "    plt.title(f\"{title} - Mel Spectrogram\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMkSsgGBwC2e"
      },
      "source": [
        "## Authenticate with HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVZ5riuKwC2e"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub.utils import HfFolder\n",
        "\n",
        "if HfFolder.get_token() is None:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL-9TSkzwC2f"
      },
      "source": [
        "## Setup HeAR Model from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6soyKJwwC2f"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import from_pretrained_keras\n",
        "\n",
        "# Load the model directly from Hugging Face Hub\n",
        "loaded_model = from_pretrained_keras(\"google/hear\")\n",
        "# Inference function for embedding generation\n",
        "infer = loaded_model.signatures[\"serving_default\"]\n",
        "\n",
        "# HeAR Parameters\n",
        "SAMPLE_RATE = 16000  # Samples per second (Hz)\n",
        "CLIP_DURATION = 2    # Duration of the audio clip in seconds\n",
        "CLIP_LENGTH = SAMPLE_RATE * CLIP_DURATION  # Total number of samples\n",
        "\n",
        "print(f\"HeAR Model loaded successfully!\")\n",
        "print(f\"Sample Rate: {SAMPLE_RATE} Hz\")\n",
        "print(f\"Clip Duration: {CLIP_DURATION} seconds\")\n",
        "print(f\"Clip Length: {CLIP_LENGTH} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE_Iq61vwC2f"
      },
      "source": [
        "## Download COUGHVID Dataset from KaggleHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnLwOWNawC2g"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version of the COUGHVID dataset\n",
        "print(\"Downloading COUGHVID dataset from KaggleHub...\")\n",
        "dataset_path = kagglehub.dataset_download(\"orvile/coughvid-v3\")\n",
        "\n",
        "print(f\"\\nDataset downloaded to: {dataset_path}\")\n",
        "print(f\"\\nDataset structure:\")\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    level = root.replace(dataset_path, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files[:5]:  # Show first 5 files per directory\n",
        "        print(f'{subindent}{file}')\n",
        "    if len(files) > 5:\n",
        "        print(f'{subindent}... and {len(files) - 5} more files')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGx0FrV2wC2h"
      },
      "source": [
        "## Load Pneumonia Labels from CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmADmMSdwC2i"
      },
      "outputs": [],
      "source": [
        "# Load the labels CSV file\n",
        "csv_path = os.path.join(dataset_path, 'tabular_form', 'tabular_form', 'filtered_expert_labels_coughvid_v3.csv')\n",
        "labels_df = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"Loaded labels from: {csv_path}\")\n",
        "print(f\"Shape: {labels_df.shape}\")\n",
        "print(f\"\\nColumn names:\")\n",
        "print(labels_df.columns.tolist())\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(labels_df.head())\n",
        "print(f\"\\nLabel value counts:\")\n",
        "if 'covid_status' in labels_df.columns:\n",
        "    print(labels_df['covid_status'].value_counts())\n",
        "if 'cough_status' in labels_df.columns:\n",
        "    print(labels_df['cough_status'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg9L96M5wC2i"
      },
      "source": [
        "## Create Pneumonia Labels Dictionary and Files Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOEWhR-6wC2i"
      },
      "outputs": [],
      "source": [
        "# Create pneumonia labels dictionary\n",
        "# Consider 'COVID-19' and 'lower_infection' as pneumonia (True)\n",
        "# Consider 'healthy' and other categories as non-pneumonia (False)\n",
        "pneumonia_labels = {}\n",
        "\n",
        "for idx, row in labels_df.iterrows():\n",
        "    # Get the filename without .json extension\n",
        "    if 'uuid' in labels_df.columns:\n",
        "        filename_base = str(row['uuid'])\n",
        "    elif 'filename' in labels_df.columns:\n",
        "        filename_base = str(row['filename']).replace('.json', '')\n",
        "    else:\n",
        "        # Use first column as filename\n",
        "        filename_base = str(row[0]).replace('.json', '')\n",
        "\n",
        "    # Determine pneumonia status based on available columns\n",
        "    has_pneumonia = False\n",
        "    if 'covid_status' in labels_df.columns:\n",
        "        covid_status = str(row['covid_status']).lower()\n",
        "        if 'covid' in covid_status or 'positive' in covid_status:\n",
        "            has_pneumonia = True\n",
        "\n",
        "    # Check for respiratory infection indicators\n",
        "    if 'cough_status' in labels_df.columns:\n",
        "        cough_status = str(row['cough_status']).lower()\n",
        "        if 'lower' in cough_status or 'infection' in cough_status:\n",
        "            has_pneumonia = True\n",
        "\n",
        "    pneumonia_labels[filename_base] = has_pneumonia\n",
        "\n",
        "# Find audio files in the dataset\n",
        "audio_extensions = ['.wav', '.ogg', '.webm', '.mp3', '.flac']\n",
        "files_map = {}\n",
        "audio_dir = os.path.join(dataset_path, 'audio_form', 'audio_form')\n",
        "\n",
        "if os.path.exists(audio_dir):\n",
        "    for filename in os.listdir(audio_dir):\n",
        "        if any(filename.endswith(ext) for ext in audio_extensions):\n",
        "            file_path = os.path.join(audio_dir, filename)\n",
        "            # Remove extension to match with labels\n",
        "            file_base = os.path.splitext(filename)[0]\n",
        "            # Only add files that have labels\n",
        "            if file_base in pneumonia_labels:\n",
        "                files_map[filename] = file_path\n",
        "\n",
        "print(f\"\\nPneumonia labels created: {len(pneumonia_labels)} entries\")\n",
        "print(f\"Pneumonia cases: {sum(pneumonia_labels.values())}\")\n",
        "print(f\"Non-pneumonia cases: {len(pneumonia_labels) - sum(pneumonia_labels.values())}\")\n",
        "print(f\"\\nAudio files found with labels: {len(files_map)}\")\n",
        "print(f\"Audio directory: {audio_dir}\")\n",
        "\n",
        "# Initialize embedding cache\n",
        "file_embeddings = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bzG6t9_wC2i"
      },
      "source": [
        "## Audio Processing Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPEAdk0CwC2j"
      },
      "outputs": [],
      "source": [
        "# Audio display and processing options\n",
        "SHOW_WAVEFORM = False\n",
        "SHOW_SPECTROGRAM = False  # Set to True to see spectrograms\n",
        "SHOW_PLAYER = False\n",
        "SHOW_CLIPS = False\n",
        "\n",
        "# Clips of length CLIP_DURATION seconds are extracted from the audio file\n",
        "# using a sliding window. Adjacent clips are overlapped by CLIP_OVERLAP_PERCENT.\n",
        "CLIP_OVERLAP_PERCENT = 10\n",
        "\n",
        "# When True, if a clip extracted from the file is quieter than\n",
        "# the SILENCE_RMS_THRESHOLD_DB it is not sent to the HeAR model.\n",
        "CLIP_IGNORE_SILENT_CLIPS = True\n",
        "# Maximum average amplitude of a frame to be considered silence.\n",
        "SILENCE_RMS_THRESHOLD_DB = -50\n",
        "\n",
        "# Limit number of files to process (set to None for all files)\n",
        "MAX_FILES_TO_PROCESS = 100  # Adjust based on your needs\n",
        "\n",
        "print(f\"Audio processing configuration:\")\n",
        "print(f\"  Clip duration: {CLIP_DURATION}s\")\n",
        "print(f\"  Clip overlap: {CLIP_OVERLAP_PERCENT}%\")\n",
        "print(f\"  Ignore silent clips: {CLIP_IGNORE_SILENT_CLIPS}\")\n",
        "print(f\"  Silence threshold: {SILENCE_RMS_THRESHOLD_DB} dB\")\n",
        "print(f\"  Max files to process: {MAX_FILES_TO_PROCESS if MAX_FILES_TO_PROCESS else 'All'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBKdBYaawC2j"
      },
      "source": [
        "## Load Audio Files and Generate HeAR Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1bqC1lqwC2j"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Process files\n",
        "files_to_process = list(files_map.items())\n",
        "if MAX_FILES_TO_PROCESS:\n",
        "    files_to_process = files_to_process[:MAX_FILES_TO_PROCESS]\n",
        "\n",
        "print(f\"Processing {len(files_to_process)} audio files...\\n\")\n",
        "\n",
        "for idx, (file_key, file_path) in enumerate(files_to_process):\n",
        "    try:\n",
        "        # Load the audio file into numpy array with specified sample rate and 1 channel (mono).\n",
        "        if (idx + 1) % 10 == 0:\n",
        "            print(f\"Processing file {idx + 1}/{len(files_to_process)}: {file_key}\")\n",
        "\n",
        "        audio, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE, mono=True)\n",
        "\n",
        "        # Display audio file (optional)\n",
        "        if SHOW_WAVEFORM:\n",
        "            plot_waveform(audio, sample_rate, title=file_key, color='blue')\n",
        "        if SHOW_SPECTROGRAM:\n",
        "            plot_spectrogram(audio, sample_rate, file_key, n_fft=2*1024, hop_length=64, n_mels=256, cmap='Blues')\n",
        "        if SHOW_PLAYER:\n",
        "            display(Audio(data=audio, rate=sample_rate))\n",
        "\n",
        "        # Segment audio into overlapping clips\n",
        "        clip_batch = []\n",
        "        overlap_samples = int(CLIP_LENGTH * (CLIP_OVERLAP_PERCENT / 100))\n",
        "        step_size = CLIP_LENGTH - overlap_samples\n",
        "        num_clips = max(1, (len(audio) - overlap_samples) // step_size)\n",
        "\n",
        "        for i in range(num_clips):\n",
        "            start_sample = i * step_size\n",
        "            end_sample = start_sample + CLIP_LENGTH\n",
        "            clip = audio[start_sample:end_sample]\n",
        "\n",
        "            # Pad clip with zeros if less than the required CLIP_LENGTH\n",
        "            if end_sample > len(audio):\n",
        "                clip = np.pad(clip, (0, CLIP_LENGTH - len(clip)), 'constant')\n",
        "\n",
        "            # Calculate average loudness of the clip (in dB)\n",
        "            rms_loudness = round(20 * np.log10(np.sqrt(np.mean(clip**2)) + 1e-10))\n",
        "\n",
        "            # Skip if clip is too quiet\n",
        "            if CLIP_IGNORE_SILENT_CLIPS and rms_loudness < SILENCE_RMS_THRESHOLD_DB:\n",
        "                continue\n",
        "\n",
        "            # Add clip to batch\n",
        "            clip_batch.append(clip)\n",
        "\n",
        "        # Perform HeAR batch inference to extract the associated clip embedding\n",
        "        if len(clip_batch) > 0:\n",
        "            clip_batch = np.asarray(clip_batch)\n",
        "            if file_key not in file_embeddings:\n",
        "                embedding_batch = infer(x=clip_batch)['output_0'].numpy()\n",
        "                file_embeddings[file_key] = embedding_batch\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_key}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nProcessing complete!\")\n",
        "print(f\"Successfully processed: {len(file_embeddings)} files\")\n",
        "print(f\"Total embeddings generated: {sum(len(emb) for emb in file_embeddings.values())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5F-XgdfwC2j"
      },
      "source": [
        "## Prepare Training and Test Sets for Pneumonia Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ir0_Z7qwC2j"
      },
      "outputs": [],
      "source": [
        "# Collect all embeddings and their corresponding labels\n",
        "all_embeddings = []\n",
        "all_labels = []\n",
        "all_file_names = []\n",
        "\n",
        "for file_key, embedding_batch in file_embeddings.items():\n",
        "    # Get the base filename without extension\n",
        "    file_base = os.path.splitext(file_key)[0]\n",
        "\n",
        "    # Get the pneumonia label for this file\n",
        "    if file_base in pneumonia_labels:\n",
        "        label = 1 if pneumonia_labels[file_base] else 0\n",
        "\n",
        "        # Add each embedding from this file\n",
        "        for embedding in embedding_batch:\n",
        "            all_embeddings.append(embedding)\n",
        "            all_labels.append(label)\n",
        "            all_file_names.append(file_key)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_embeddings = np.array(all_embeddings)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "print(f\"Total embeddings: {len(all_embeddings)}\")\n",
        "print(f\"Total pneumonia cases: {sum(all_labels)}\")\n",
        "print(f\"Total non-pneumonia cases: {len(all_labels) - sum(all_labels)}\")\n",
        "print(f\"\\nEmbedding shape: {all_embeddings.shape}\")\n",
        "print(f\"Labels shape: {all_labels.shape}\")\n",
        "\n",
        "# Split into train and test sets with stratification\n",
        "try:\n",
        "    train_embeddings, test_embeddings, train_labels, test_labels = train_test_split(\n",
        "        all_embeddings, all_labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=all_labels\n",
        "    )\n",
        "    print(f\"\\nTrain/test split with stratification successful!\")\n",
        "except ValueError as e:\n",
        "    print(f\"\\nStratification not possible: {e}\")\n",
        "    print(\"Performing split without stratification...\")\n",
        "    train_embeddings, test_embeddings, train_labels, test_labels = train_test_split(\n",
        "        all_embeddings, all_labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "print(f\"\\nTraining set:\")\n",
        "print(f\"  Total samples: {len(train_embeddings)}\")\n",
        "print(f\"  Pneumonia cases: {sum(train_labels)}\")\n",
        "print(f\"  Non-pneumonia cases: {len(train_labels) - sum(train_labels)}\")\n",
        "\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"  Total samples: {len(test_embeddings)}\")\n",
        "print(f\"  Pneumonia cases: {sum(test_labels)}\")\n",
        "print(f\"  Non-pneumonia cases: {len(test_labels) - sum(test_labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2t7BNX2wC2j"
      },
      "source": [
        "## Train Pneumonia Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbnPo1-twC2j"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Define multiple classifier models\n",
        "models = {\n",
        "    \"Support Vector Machine (linear)\": SVC(kernel='linear', probability=True),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=128),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=128, random_state=42),\n",
        "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=1000, random_state=42),\n",
        "}\n",
        "\n",
        "# Train each model\n",
        "pneumonia_models = {}\n",
        "print(\"Training pneumonia classifiers...\\n\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Training: {name}\")\n",
        "    model.fit(train_embeddings, train_labels)\n",
        "    pneumonia_models[name] = model\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = model.score(train_embeddings, train_labels)\n",
        "    print(f\"  Training accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "print(f\"\\nAll {len(pneumonia_models)} models trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm8Zl2bgwC2j"
      },
      "source": [
        "## Evaluate Pneumonia Classifiers on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcjiv9-dwC2j"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(f\"Evaluating {len(pneumonia_models)} models on test set ({len(test_embeddings)} samples)...\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for model_name, pneumonia_model in pneumonia_models.items():\n",
        "    # Make predictions\n",
        "    predictions = pneumonia_model.predict(test_embeddings)\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_accuracy = accuracy_score(test_labels, predictions)\n",
        "\n",
        "    print(f\"\\n{model_name}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(test_labels, predictions,\n",
        "                                target_names=['No Pneumonia', 'Pneumonia'],\n",
        "                                zero_division=0))\n",
        "    print(f\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(test_labels, predictions))\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW-UUZ2mwC2j"
      },
      "source": [
        "## Visualize Embeddings with PCA (Pneumonia Status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKlxOm7qwC2j"
      },
      "outputs": [],
      "source": [
        "# Fit PCA on all embeddings\n",
        "pca = PCA(n_components=2)\n",
        "all_embeddings_pca = pca.fit_transform(all_embeddings)\n",
        "\n",
        "# Separate by pneumonia status\n",
        "pneumonia_mask = all_labels == 1\n",
        "no_pneumonia_mask = all_labels == 0\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot non-pneumonia cases\n",
        "plt.scatter(all_embeddings_pca[no_pneumonia_mask, 0],\n",
        "           all_embeddings_pca[no_pneumonia_mask, 1],\n",
        "           color='blue', alpha=0.5, label='No Pneumonia', s=50)\n",
        "\n",
        "# Plot pneumonia cases\n",
        "plt.scatter(all_embeddings_pca[pneumonia_mask, 0],\n",
        "           all_embeddings_pca[pneumonia_mask, 1],\n",
        "           color='red', alpha=0.5, label='Pneumonia', s=50)\n",
        "\n",
        "plt.xlabel(f\"PCA Dimension 1 ({pca.explained_variance_ratio_[0]:.2%} variance)\")\n",
        "plt.ylabel(f\"PCA Dimension 2 ({pca.explained_variance_ratio_[1]:.2%} variance)\")\n",
        "plt.title(\"HeAR Embeddings Visualization - Pneumonia Classification\\n(PCA Projection)\")\n",
        "plt.legend(loc='best', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Total variance explained by 2 components: {sum(pca.explained_variance_ratio_):.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPuSkq4owC2k"
      },
      "source": [
        "## Visualize Sample Embeddings as Barcodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl5S47z3wC2k"
      },
      "outputs": [],
      "source": [
        "# Select a few sample files to visualize (limit to first 5 files)\n",
        "embedding_mean = np.mean(all_embeddings, axis=0)\n",
        "sample_files = list(file_embeddings.keys())[:5]\n",
        "\n",
        "print(f\"Visualizing embeddings for {len(sample_files)} sample files...\\n\")\n",
        "\n",
        "for file_key in sample_files:\n",
        "    embedding_batch = file_embeddings[file_key]\n",
        "    batch_size = embedding_batch.shape[0]\n",
        "\n",
        "    # Get pneumonia status\n",
        "    file_base = os.path.splitext(file_key)[0]\n",
        "    is_pneumonia = pneumonia_labels.get(file_base, False)\n",
        "    status = \"PNEUMONIA\" if is_pneumonia else \"NO PNEUMONIA\"\n",
        "\n",
        "    # Subtract mean for visualization\n",
        "    embedding_batch_norm = embedding_batch - embedding_mean\n",
        "\n",
        "    print(f\"{file_key} - {status} ({batch_size} embeddings)\")\n",
        "\n",
        "    plt.figure(figsize=(18, 1 * batch_size))\n",
        "    for i in range(batch_size):\n",
        "        embedding_magnitude = embedding_batch_norm[i, :] ** 2\n",
        "        plt.subplot(batch_size, 1, i + 1)\n",
        "        plt.imshow(embedding_magnitude.reshape(1, -1), cmap='binary', interpolation=None, aspect='auto')\n",
        "        plt.title(f\"Embedding {i+1}/{batch_size} - {file_key} - {status}\", fontsize=10)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHTxQttKYNpa"
      },
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "This notebook has successfully transformed audio classification from cough detection to **pneumonia classification** using:\n",
        "\n",
        "- **Dataset**: COUGHVID-v3 from KaggleHub (orvile/coughvid-v3)\n",
        "- **Model**: Google HeAR (Health Acoustic Representations)\n",
        "- **Labels**: Pneumonia cases identified from COVID-19 and lower respiratory infections\n",
        "- **Classifiers**: SVM, Logistic Regression, Gradient Boosting, Random Forest, MLP\n",
        "\n",
        "### Key Results:\n",
        "- Processed audio files and generated HeAR embeddings\n",
        "- Trained multiple classifiers on pneumonia vs. non-pneumonia cases\n",
        "- Evaluated performance on held-out test set\n",
        "- Visualized embedding space to show class separation\n",
        "\n",
        "### Next Steps:\n",
        "1. **Tune hyperparameters** for better model performance\n",
        "2. **Balance dataset** if class imbalance is significant\n",
        "3. **Add more features** or try ensemble methods\n",
        "4. **Deploy models** for real-world pneumonia screening\n",
        "5. **Validate** on external datasets\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/hear/blob/master/notebooks) to learn what else you can do with the HeAR model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "train_data_efficient_classifier.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}